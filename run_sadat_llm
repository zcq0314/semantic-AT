from __future__ import absolute_import, division, print_function
import argparse
import csv
import logging
import os
import random
import math
import numpy as np
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import sklearn.metrics as mtc
from scipy.stats import spearmanr
from tqdm import tqdm, trange
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoConfig,
    SchedulerType, 
    get_scheduler,
    BitsAndBytesConfig
)

try:
    from peft import (
        get_peft_model, 
        LoraConfig, 
        TaskType,
        prepare_model_for_kbit_training
    )
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("Warning: PEFT library not found. LoRA will not be available.")

from sadat_qwen import QwenSadatTrainer, Trainer

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO
)
logger = logging.getLogger(__name__)


class InputExample:
    def __init__(self, guid, text_a, text_b=None, label=None):
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


class InputFeatures:
    def __init__(self, input_ids, input_mask, label_id):
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.label_id = label_id


class DataProcessor:
    def get_train_examples(self, data_dir):
        raise NotImplementedError()

    def get_dev_examples(self, data_dir):
        raise NotImplementedError()

    def get_labels(self):
        raise NotImplementedError()

    @classmethod
    def _read_tsv(cls, input_file, quotechar=None):
        with open(input_file, 'r', encoding="utf-8") as f:
            reader = csv.reader(f, delimiter='\t', quotechar=quotechar)
            lines = []
            for line in reader:
                lines.append(line)
            return lines
    
    @classmethod
    def _read_json(cls, input_file):
        import json
        lines = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                lines.append(json.loads(line.strip()))
        return lines


class TnewsProcessor(DataProcessor):
    def get_train_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        return ['100', '101', '102', '103', '104', '106', '107', 
                '108', '109', '110', '112', '113', '114', '115', '116']

    @staticmethod
    def _create_examples(lines, set_type):
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            if set_type == "test":
                text_a = line[1] if len(line) > 1 else ""
                label = '100'
            else:
                text_a = line[2] if len(line) > 2 else line[1]
                label = line[0]
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        return examples


class AfqmcProcessor(DataProcessor):

    def get_train_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        return ['0', '1']

    @staticmethod
    def _create_examples(lines, set_type):
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            if set_type == "test":
                text_a = line[-2] if len(line) >= 2 else ""
                text_b = line[-1] if len(line) >= 1 else ""
                label = '0'
            else:
                text_a = line[-3] if len(line) >= 3 else ""
                text_b = line[-2] if len(line) >= 2 else ""
                label = line[-1] if len(line) >= 1 else '0'
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class CMnliProcessor(DataProcessor):

    def get_train_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        return ["contradiction", "entailment", "neutral"]

    @staticmethod
    def _create_examples(lines, set_type):
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0] if len(line) > 0 else i)
            if set_type == "test":
                text_a = line[-2] if len(line) >= 2 else ""
                text_b = line[-1] if len(line) >= 1 else ""
                label = "contradiction"
            else:
                text_a = line[-3] if len(line) >= 3 else ""
                text_b = line[-2] if len(line) >= 2 else ""
                label = line[-1] if len(line) >= 1 else "contradiction"
                # 过滤掉标签为"-"的样本
                if label == "-":
                    continue
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class OCnliProcessor(DataProcessor):

    def get_train_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        return ["contradiction", "entailment", "neutral"]

    @staticmethod
    def _create_examples(lines, set_type):
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, line[0] if len(line) > 0 else i)
            if set_type == "test":
                text_a = line[-3] if len(line) >= 3 else ""
                text_b = line[-2] if len(line) >= 2 else ""
                label = "contradiction"
            else:
                text_a = line[1] if len(line) > 1 else ""
                text_b = line[2] if len(line) > 2 else ""
                label = line[3] if len(line) > 3 else "contradiction"
                # 过滤掉标签为"-"的样本
                if label == "-":
                    continue
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples


class IflytekProcessor(DataProcessor):
    def get_train_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

    def get_test_examples(self, data_dir):
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self):
        return [str(i) for i in range(119)]

    @staticmethod
    def _create_examples(lines, set_type):
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            if set_type == "test":
                text_a = line[1] if len(line) > 1 else ""
                label = '0'
            else:
                text_a = line[2] if len(line) > 2 else line[1] if len(line) > 1 else ""
                label = line[0] if len(line) > 0 else '0'
            examples.append(
                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        return examples


def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):
    
    label_map = {label: i for i, label in enumerate(label_list)}
    features = []
    
    for i, example in enumerate(examples):
        if example.text_b:
            text = f"{example.text_a} [SEP] {example.text_b}"
            encoded_inputs = tokenizer(
                text,
                max_length=max_seq_length,
                padding="max_length",
                truncation=True,
                return_tensors=None
            )
        else:
            encoded_inputs = tokenizer(
                example.text_a,
                max_length=max_seq_length,
                padding="max_length",
                truncation=True,
                return_tensors=None
            )
        
        input_ids = encoded_inputs["input_ids"]
        input_mask = encoded_inputs["attention_mask"]

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length

        label_id = label_map.get(example.label, 0)
        
        if i < 3:
            logger.info("*** Example ***")
            logger.info("guid: %s" % example.guid)
            logger.info("input_ids: %s" % ' '.join([str(x) for x in input_ids[:20]]) + "...")
            logger.info("label: %s (id = %s)" % (example.label, label_id))

        features.append(
            InputFeatures(
                input_ids=input_ids,
                input_mask=input_mask,
                label_id=label_id
            )
        )

    return features


class Metrics:
    @staticmethod
    def acc(predictions, labels):
        return mtc.accuracy_score(labels, predictions)

    @staticmethod
    def mcc(predictions, labels):
        return mtc.matthews_corrcoef(labels, predictions)

    @staticmethod
    def f1(predictions, labels, average="micro"):
        return mtc.f1_score(labels, predictions, average=average)

def setup_lora(model, lora_r=8, lora_alpha=16, lora_dropout=0.1, target_modules=None):

    if not PEFT_AVAILABLE:
        logger.warning("PEFT not available, returning original model")
        return model
    
    if target_modules is None:
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ]
    
    lora_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        inference_mode=False
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default="data/",
                        help="数据目录路径")
    parser.add_argument("--task_name", type=str, default="afqmc",
                        choices=["tnews", "afqmc", "cmnli", "ocnli", "iflytek"],
                        help="任务名称")
    parser.add_argument("--model_name_or_path", type=str, 
                        default="qwen",
                        help="预训练模型路径或名称")
    parser.add_argument("--cache_dir", type=str, default="./cache/",
                        help="模型缓存目录")
    parser.add_argument("--output_dir", type=str, default="./output/",
                        help="输出目录")
    parser.add_argument("--do_train", action="store_true", default=True,
                        help="是否训练")
    parser.add_argument("--do_eval", action="store_true", default=True,
                        help="是否评估")
    parser.add_argument("--do_test", action="store_true",
                        help="是否测试")
    parser.add_argument("--max_seq_length", type=int, default=128,
                        help="最大序列长度")
    parser.add_argument("--train_batch_size", type=int, default=8,
                        help="训练批次大小")
    parser.add_argument("--eval_batch_size", type=int, default=16,
                        help="评估批次大小")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                        help="学习率（LoRA通常使用较高学习率）")
    parser.add_argument("--num_train_epochs", type=float, default=3.0,
                        help="训练轮数")
    parser.add_argument("--max_train_steps", type=int, default=None,
                        help="最大训练步数")
    parser.add_argument("--lr_scheduler_type", type=SchedulerType, default="cosine",
                        help="学习率调度器类型")
    parser.add_argument("--warmup_proportion", type=float, default=0.1,
                        help="预热比例")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                        help="权重衰减")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                        help="梯度累积步数")
    parser.add_argument("--seed", type=int, default=42,
                        help="随机种子")
    parser.add_argument("--fp16", action="store_true",
                        help="使用FP16混合精度")
    parser.add_argument("--bf16", action="store_true", default=True,
                        help="使用BF16混合精度（推荐用于Qwen）")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=True,
                        help="使用梯度检查点节省显存")
    parser.add_argument("--load_in_8bit", action="store_true",
                        help="8位量化加载模型")
    parser.add_argument("--load_in_4bit", action="store_true",
                        help="4位量化加载模型")
    parser.add_argument("--use_lora", action="store_true", default=True,
                        help="是否使用LoRA")
    parser.add_argument("--lora_r", type=int, default=8,
                        help="LoRA秩")
    parser.add_argument("--lora_alpha", type=int, default=16,
                        help="LoRA缩放因子")
    parser.add_argument("--lora_dropout", type=float, default=0.1,
                        help="LoRA Dropout")
    parser.add_argument("--use_sadat", action="store_true", default=True,
                        help="是否使用sadat对抗训练")
    parser.add_argument("--adv_steps", type=int, default=2,
                        help="对抗训练迭代次数")
    parser.add_argument("--adv_lr", type=float, default=5e-2,
                        help="对抗训练学习率")
    parser.add_argument("--adv_max_norm", type=float, default=5e-2,
                        help="对抗扰动最大范数")
    parser.add_argument("--adv_temp", type=float, default=1.0,
                        help="对抗训练温度参数")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                        help="从checkpoint恢复训练的路径（LoRA adapter目录或.bin文件）")
    parser.add_argument("--resume_epoch", type=int, default=0,
                        help="从第几个epoch开始继续训练")

    args = parser.parse_args()
    processors = {
        "tnews": TnewsProcessor,
        "afqmc": AfqmcProcessor,
        "cmnli": CMnliProcessor,
        "ocnli": OCnliProcessor,
        "iflytek": IflytekProcessor,
    }

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    n_gpu = torch.cuda.device_count()
    logger.info(f"Device: {device}, n_gpu: {n_gpu}")
    logger.info(f"BF16: {args.bf16}, FP16: {args.fp16}")

    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if n_gpu > 0:
        torch.cuda.manual_seed_all(args.seed)

    task_output_dir = os.path.join(args.output_dir, args.task_name)
    if not os.path.exists(task_output_dir):
        os.makedirs(task_output_dir)

    task_name = args.task_name.lower()
    if task_name not in processors:
        raise ValueError(f"Task not found: {task_name}")

    processor = processors[task_name]()
    label_list = processor.get_labels()
    num_labels = len(label_list)
    logger.info(f"Task: {task_name}, Num labels: {num_labels}")

    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        cache_dir=args.cache_dir,
        trust_remote_code=True
    )
    
    tokenizer.padding_side = "right" 
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    quantization_config = None
    if args.load_in_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16 if args.bf16 else torch.float16,
            bnb_4bit_use_double_quant=True
        )
    elif args.load_in_8bit:
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)

    if args.do_train:
        train_examples = processor.get_train_examples(
            os.path.join(args.data_dir, task_name)
        )
        train_features = convert_examples_to_features(
            train_examples, label_list, args.max_seq_length, tokenizer
        )

        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)
        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)

        all_segment_ids = torch.zeros_like(all_input_ids)

        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)
        train_sampler = RandomSampler(train_data)
        train_dataloader = DataLoader(
            train_data, 
            sampler=train_sampler, 
            batch_size=args.train_batch_size
        )

        num_update_steps_per_epoch = math.ceil(
            len(train_dataloader) / args.gradient_accumulation_steps
        )
        if args.max_train_steps is None:
            args.max_train_steps = int(args.num_train_epochs * num_update_steps_per_epoch)
        else:
            args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

        logger.info(f"Loading model from {args.model_name_or_path}")
        
        config = AutoConfig.from_pretrained(
            args.model_name_or_path,
            num_labels=num_labels,
            cache_dir=args.cache_dir,
            trust_remote_code=True
        )
        
        model = AutoModelForSequenceClassification.from_pretrained(
            args.model_name_or_path,
            config=config,
            cache_dir=args.cache_dir,
            trust_remote_code=True,
            quantization_config=quantization_config,
            torch_dtype=torch.bfloat16 if args.bf16 else (torch.float16 if args.fp16 else torch.float32),
            device_map="auto" if quantization_config else None
        )
        
        if model.config.pad_token_id is None:
            model.config.pad_token_id = tokenizer.pad_token_id

        if args.gradient_checkpointing:
            model.gradient_checkpointing_enable()
            logger.info("Gradient checkpointing enabled")
        if args.use_lora and PEFT_AVAILABLE:
            if args.resume_from_checkpoint and os.path.isdir(args.resume_from_checkpoint):
                # 从已保存的LoRA adapter恢复
                from peft import PeftModel
                logger.info(f"Loading LoRA adapter from checkpoint: {args.resume_from_checkpoint}")
                model = PeftModel.from_pretrained(
                    model,
                    args.resume_from_checkpoint,
                    is_trainable=True
                )
                logger.info(f"Loaded LoRA adapter from {args.resume_from_checkpoint}")
            else:
                if quantization_config:
                    model = prepare_model_for_kbit_training(model)
                model = setup_lora(
                    model, 
                    lora_r=args.lora_r,
                    lora_alpha=args.lora_alpha,
                    lora_dropout=args.lora_dropout
                )
                logger.info("LoRA applied to model")
        elif args.resume_from_checkpoint:
            if args.resume_from_checkpoint.endswith('.bin'):
                logger.info(f"Loading model weights from: {args.resume_from_checkpoint}")
                state_dict = torch.load(args.resume_from_checkpoint, map_location=device)
                model.load_state_dict(state_dict)
                logger.info(f"Loaded model weights from {args.resume_from_checkpoint}")

        if not quantization_config:
            model.to(device)

        no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() 
                          if p.requires_grad and not any(nd in n for nd in no_decay)],
                "weight_decay": args.weight_decay
            },
            {
                "params": [p for n, p in model.named_parameters() 
                          if p.requires_grad and any(nd in n for nd in no_decay)],
                "weight_decay": 0.0
            }
        ]

        optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters, 
            lr=args.learning_rate
        )
        
        scheduler = get_scheduler(
            name=args.lr_scheduler_type,
            optimizer=optimizer,
            num_warmup_steps=int(args.max_train_steps * args.warmup_proportion),
            num_training_steps=args.max_train_steps
        )

        if args.do_eval:
            eval_examples = processor.get_dev_examples(
                os.path.join(args.data_dir, task_name)
            )
            eval_features = convert_examples_to_features(
                eval_examples, label_list, args.max_seq_length, tokenizer
            )

            all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)
            all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)
            all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)
            all_segment_ids = torch.zeros_like(all_input_ids)

            eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)
            eval_sampler = SequentialSampler(eval_data)
            eval_dataloader = DataLoader(
                eval_data, 
                sampler=eval_sampler, 
                batch_size=args.eval_batch_size
            )

        logger.info("***** Running training *****")
        logger.info(f"  Num examples = {len(train_examples)}")
        logger.info(f"  Batch size = {args.train_batch_size}")
        logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")
        logger.info(f"  Num steps = {args.max_train_steps}")
        logger.info(f"  Using sadat = {args.use_sadat}")
        logger.info(f"  Using LoRA = {args.use_lora}")

        if args.use_sadat:
            trainer = QwenSadatTrainer(
                model=model,
                optimizer=optimizer,
                tokenizer=tokenizer,
                scheduler=scheduler,
                max_train_steps=args.max_train_steps,
                gradient_accumulation_steps=args.gradient_accumulation_steps,
                fp16=args.fp16,
                bf16=args.bf16,
                adv_steps=args.adv_steps,
                adv_lr=args.adv_lr,
                adv_max_norm=args.adv_max_norm,
                adv_temp=args.adv_temp
            )
        else:
            trainer = Trainer(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                max_train_steps=args.max_train_steps,
                gradient_accumulation_steps=args.gradient_accumulation_steps,
                fp16=args.fp16,
                bf16=args.bf16
            )

        # 训练循环
        best_epoch = 0
        best_result = 0.0
        start_epoch = args.resume_epoch if args.resume_from_checkpoint else 0
        
        for epoch in trange(start_epoch, int(args.num_train_epochs), desc="Epoch"):
            # 训练一个epoch
            train_loss, train_step, average_similarity, average_perplexity = trainer.step(
                train_dataloader
            )
            global_step = trainer.global_step

            # 保存模型
            output_model_file = os.path.join(task_output_dir, f"epoch_{epoch}")
            model_to_save = model.module if hasattr(model, "module") else model
            
            if args.use_lora and PEFT_AVAILABLE:
                model_to_save.save_pretrained(output_model_file)
            else:
                torch.save(model_to_save.state_dict(), f"{output_model_file}.bin")

            # 评估
            if args.do_eval:
                logger.info("***** Running evaluation *****")
                logger.info(f"  Num examples = {len(eval_examples)}")
                
                model.eval()
                eval_loss = 0
                eval_steps = 0
                all_predictions, all_labels = [], []

                for batch in tqdm(eval_dataloader, desc="Evaluation"):
                    batch = tuple(t.to(device) for t in batch)
                    input_ids, input_mask, segment_ids, label_ids = batch
                    
                    with torch.no_grad():
                        outputs = model(
                            input_ids=input_ids,
                            attention_mask=input_mask,
                            labels=label_ids
                        )
                        tmp_eval_loss = outputs.loss
                        logits = outputs.logits

                    logits = logits.detach().float().cpu().numpy()
                    label_ids = label_ids.cpu().numpy()
                    
                    eval_loss += tmp_eval_loss.mean().item()
                    
                    predicted_labels = np.argmax(logits, axis=1)
                    all_predictions.extend(predicted_labels.tolist())
                    all_labels.extend(label_ids.tolist())
                    
                    eval_steps += 1

                model.train()
                loss = train_loss / train_step if train_step > 0 else 0
                eval_loss = eval_loss / eval_steps if eval_steps > 0 else 0
                eval_acc = Metrics.acc(all_predictions, all_labels) * 100

                result = {
                    "epoch": epoch,
                    "global_step": global_step,
                    "train_loss": loss,
                    "eval_loss": eval_loss,
                    "eval_acc": eval_acc,
                    "average_similarity": average_similarity,
                    "average_perplexity": average_perplexity,
                }

                if eval_acc > best_result:
                    best_epoch = epoch
                    best_result = eval_acc
                output_eval_file = os.path.join(task_output_dir, "eval_results.txt")
                with open(output_eval_file, 'a') as writer:
                    writer.write(
                        f"Epoch {epoch}: global_step={global_step} | "
                        f"train_loss={loss:.4f} | eval_acc={eval_acc:.2f} | "
                        f"eval_loss={eval_loss:.4f} | "
                        f"similarity={average_similarity:.4f} | "
                        f"perplexity={average_perplexity:.4f}\n"
                    )

                logger.info(f"Epoch {epoch}: eval_acc = {eval_acc:.2f}")

        logger.info(f"Best epoch: {best_epoch}, Best accuracy: {best_result:.2f}")

    if args.do_test:
        logger.info("***** Running test *****")
        
        test_examples = processor.get_test_examples(
            os.path.join(args.data_dir, task_name)
        )
        test_features = convert_examples_to_features(
            test_examples, label_list, args.max_seq_length, tokenizer
        )

        all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)
        all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)

        test_data = TensorDataset(all_input_ids, all_input_mask)
        test_sampler = SequentialSampler(test_data)
        test_dataloader = DataLoader(
            test_data, 
            sampler=test_sampler, 
            batch_size=args.eval_batch_size
        )

        model.eval()
        predictions = []

        for batch in tqdm(test_dataloader, desc="Testing"):
            input_ids = batch[0].to(device)
            input_mask = batch[1].to(device)
            
            with torch.no_grad():
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=input_mask
                )
                logits = outputs.logits

            logits = logits.detach().float().cpu().numpy()
            predicted_labels = np.argmax(logits, axis=1)
            predictions.extend(predicted_labels.tolist())

        output_test_file = os.path.join(task_output_dir, f"{task_name}_predictions.tsv")
        with open(output_test_file, 'w') as writer:
            writer.write("index\tprediction\n")
            for index, pred in enumerate(predictions):
                writer.write(f"{index}\t{label_list[pred]}\n")

        logger.info(f"Test predictions saved to {output_test_file}")


if __name__ == "__main__":
    main()
